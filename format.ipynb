{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"format.ipynb","provenance":[{"file_id":"1Se6dQ-kG0XuNC1ETlsF_t0vN0EANwWIw","timestamp":1643550903476}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In this notebook we will take the data from the [SIGMORPHON 2018 1. task](https://github.com/sigmorphon/conll2018/tree/master/task1/all) and we will format it to be able to feed it to the [Fairseq](https://github.com/pytorch/fairseq) functions."],"metadata":{"id":"yrI3SYfnBVru"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_mRsGQF5klk","executionInfo":{"status":"ok","timestamp":1643648229756,"user_tz":-60,"elapsed":16563,"user":{"displayName":"Jon Ander GonzÃ¡lez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11218400420728313598"}},"outputId":"0084b902-27ee-4c4b-c972-92d6e6352fe9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/backtranslation\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/My Drive/Colab Notebooks/backtranslation/\""]},{"cell_type":"code","source":["import pandas as pd\n","import os"],"metadata":{"id":"iCSwUwLo5toU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data is in csv format, with no headers and separated by tabulation. The 1. column is the word in lemma form, the 2. column is the word inflected and the 3. column is the tags that describe the inflection information. Each row of the data is a an example. The examples present on the dataset are only verbs.\n","\n","We will use the datasets for Basque and English. Each one contains three training sets with different number of examples --low (100), medium (1000) and high (10000)-- a dev set and a test set."],"metadata":{"id":"5QDQe78b6ERp"}},{"cell_type":"code","source":["INPUT = os.path.join('./data', 'sigmorphon')\n","\n","es_train_low = os.path.join(INPUT, 'spanish-train-low.txt')\n","es_train_med = os.path.join(INPUT, 'spanish-train-medium.txt')\n","es_gen = os.path.join(INPUT, 'spanish-train-high.txt')\n","es_dev = os.path.join(INPUT, 'spanish-dev.txt')\n","es_test = os.path.join(INPUT, 'spanish-test.txt')\n","\n","eu_train_low = os.path.join(INPUT, 'basque-train-low.txt')\n","eu_train_med = os.path.join(INPUT, 'basque-train-medium.txt')\n","eu_gen = os.path.join(INPUT, 'basque-train-high.txt')\n","eu_dev = os.path.join(INPUT, 'basque-dev.txt')\n","eu_test = os.path.join(INPUT, 'basque-test.txt')"],"metadata":{"id":"Y9KwNZrd74hv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From these, new datasets with the correct format will be made. Three training datasets -- low (100) -> low (100), med (1000) -> med (500), med (1000) -> high (1000)--and a dataset for backtranslation -- high (10000) -> gen (5000). The test and dev datasets will contain exactly the same data."],"metadata":{"id":"t5E3PEVsF2B7"}},{"cell_type":"code","source":["df_es_train_low = pd.read_csv(es_train_low, sep=\"\\t\", header=None)\n","df_es_train_high = pd.read_csv(es_train_med, sep=\"\\t\", header=None)\n","df_es_train_med = df_es_train_high.sample(frac=0.5)\n","df_es_gen = pd.read_csv(es_gen, sep=\"\\t\", header=None)\n","df_es_gen = df_es_gen.sample(frac=0.5)\n","df_es_dev = pd.read_csv(es_dev, sep=\"\\t\", header=None)\n","df_es_test = pd.read_csv(es_test, sep=\"\\t\", header=None)\n","\n","df_eu_train_low = pd.read_csv(eu_train_low, sep=\"\\t\", header=None)\n","df_eu_train_high = pd.read_csv(eu_train_med, sep=\"\\t\", header=None)\n","df_eu_train_med = df_eu_train_high.sample(frac=0.5)\n","df_eu_gen = pd.read_csv(eu_gen, sep=\"\\t\", header=None)\n","df_es_gen = df_es_gen.sample(frac=0.5)\n","df_eu_dev = pd.read_csv(eu_dev, sep=\"\\t\", header=None)\n","df_eu_test = pd.read_csv(eu_test, sep=\"\\t\", header=None)"],"metadata":{"id":"rNjl1FaE51fB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For each dataset, the three columns will be transformed in 2 different files. The first one will contain the lemmas with the tags. The second, will contain the inflected words. \n","\n","Since the inflection and tagger models will use characters as tokens, we need to separate each character in the words, and each inflection information in the tag."],"metadata":{"id":"65M1eCzkHBMY"}},{"cell_type":"code","source":["def format(dataset):\n","    lemmas = dataset.iloc[:,0]\n","    tags = dataset.iloc[:,2]\n","    lemmas = [\"<\" + lemma + \">\" for lemma in lemmas]\n","    lemmas = [\" \".join(lemma) for lemma in lemmas]\n","    tags = [tag.replace(\";\", \" \") for tag in tags]\n","    lemmas = [lemma + \" \" + tag for (lemma, tag) in zip(lemmas,tags)]\n","\n","    inflecteds = dataset.iloc[:,1]\n","    inflecteds = [inflected.replace(\" \", \"#\") for inflected in inflecteds]\n","    inflecteds = [\"<\" + inflected + \">\" for inflected in inflecteds]\n","    inflecteds = [\" \".join(inflected) for inflected in inflecteds]\n","\n","    return lemmas, inflecteds"],"metadata":{"id":"wCo-x49q96k6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process(dataset, t, lang):\n","    path_output = os.path.join(OUTPUT, t)\n","    if not os.path.exists(path_output):\n","        os.makedirs(path_output)\n","    if t in ['low', 'med', 'high']:\n","        path_output_lemma = os.path.join(path_output, 'train.{}.lemma_tag'.format(lang))\n","        path_output_inflected = os.path.join(path_output, 'train.{}.inflected'.format(lang))\n","    else:\n","        path_output_lemma = os.path.join(path_output, '{}.{}.lemma_tag'.format(t, lang))\n","        path_output_inflected = os.path.join(path_output, '{}.{}.inflected'.format(t, lang))\n","    \n","    lemmas, inflecteds = format(dataset)\n","\n","    with open(path_output_lemma, \"w\") as f:\n","        f.write(\"\\n\".join(lemmas))\n","\n","    with open(path_output_inflected, \"w\") as f:\n","        f.write(\"\\n\".join(inflecteds))"],"metadata":{"id":"ca3qH1yAgQ-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OUTPUT = os.path.join('./data', 'prepared')\n","types = ['low', 'med', 'high', 'dev', 'gen', 'test']\n","langs = ['es','eu']"],"metadata":{"id":"kdwmbb8TvoG2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets_es = [df_es_train_low, df_es_train_med, df_es_train_high, df_es_dev, df_es_gen, df_es_test]\n","datasets_eu = [df_eu_train_low, df_eu_train_med, df_eu_train_high, df_eu_dev, df_eu_gen, df_eu_test]\n","datasets = [datasets_es, datasets_eu]"],"metadata":{"id":"smtMZXkjhdNh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for (lang_datasets,lang) in zip(datasets, langs):\n","    for (d, t) in zip(lang_datasets, types):\n","        process(d, t, lang)"],"metadata":{"id":"R_OBKpa9T3UE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"8HRzfRNEEVQl"},"execution_count":null,"outputs":[]}]}